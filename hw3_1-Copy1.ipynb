{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "class MLP:\n",
    "    \n",
    "    def __init__(self, train_file, test_file=None):\n",
    "        # extract training data\n",
    "        data = self.read_data(train_file)\n",
    "        # split data into training and validation\n",
    "        self.training = {}\n",
    "        self.test = {}\n",
    "        self.validation = {}\n",
    "#         p = np.random.permutation(data['xdata'].shape[0])\n",
    "        if test_file is None:\n",
    "            self.training['xdata'], self.validation['xdata'], _ = np.split(data['xdata'] / 255, [50000, 60000])\n",
    "            self.training['ydata'], self.validation['ydata'], _ = np.split(data['ydata'], [50000, 60000])\n",
    "        else:\n",
    "            t_data = self.read_data(test_file)\n",
    "            self.training['xdata'] = data['xdata'] / 255\n",
    "            self.training['ydata'] = data['ydata']\n",
    "            self.test['xdata'] = t_data['xdata'] / 255\n",
    "            self.test['ydata'] = t_data['ydata']\n",
    "            print(self.training['xdata'].shape)\n",
    "            print(self.test['xdata'].shape)\n",
    "            print(self.training['ydata'].shape)\n",
    "            print(self.test['ydata'].shape)\n",
    "    \n",
    "    def read_data(self, filename):\n",
    "        data = {}\n",
    "        with h5py.File(filename, 'r') as hf:\n",
    "            for k in hf.keys():\n",
    "                data[k] = hf[k][:]\n",
    "        return data\n",
    "    \n",
    "    class layer:\n",
    "        def __init__(self, input_size, output_size, activate='ReLU', regularizer=None):\n",
    "            self.weight = np.random.normal(0, 0.1, (input_size, output_size))\n",
    "            self.bias = np.random.normal(0, 0.1, (output_size, ))\n",
    "            self.activate = activate\n",
    "            self.regularizer = regularizer\n",
    "            \n",
    "        def set_par(self, w, b):\n",
    "            # for testing\n",
    "            self.weight = w[:]\n",
    "            self.bias = b[:]\n",
    "        \n",
    "        def tanh(self, x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def ReLU(self, x):\n",
    "            return np.where(x > 0.0, x, 0.0)\n",
    "        \n",
    "        def softmax(self, x):\n",
    "            x = x - np.max(x, axis=1).reshape(x.shape[0], 1)\n",
    "            return (np.exp(x) / np.sum(np.exp(x), axis=1).reshape(x.shape[0], 1))\n",
    "#             return np.exp(x - np.mean(x)) / np.sum(np.exp(x - np.mean(x)))\n",
    "#             return (np.exp(x).T / np.sum(np.exp(x), axis=1)).T\n",
    "#             tmp = np.exp(x - np.max(x))\n",
    "#             return tmp / np.sum(tmp)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            self.x = x[:]\n",
    "            self.s = self.x @ self.weight + self.bias\n",
    "            if self.activate == 'tanh':\n",
    "                return self.tanh(self.s)\n",
    "            elif self.activate == 'ReLU':\n",
    "                return self.ReLU(self.s)\n",
    "            elif self.activate == 'softmax':\n",
    "                return self.softmax(self.s)\n",
    "\n",
    "        def backward(self, delta):\n",
    "            self.delta = delta[:]\n",
    "            if self.activate == 'tanh':\n",
    "                self.delta *= (1 - np.square(self.tanh(self.s)))\n",
    "            elif self.activate == 'ReLU':\n",
    "                self.delta *= np.where(self.s >= 0.0, 1.0, 0.0)\n",
    "            elif self.activate == 'softmax':\n",
    "                pass\n",
    "            d = self.delta @ self.weight.T\n",
    "            return d\n",
    "        \n",
    "        def update(self, eta):\n",
    "#             print(self.x.reshape((self.x.shape[0], -1)).shape)\n",
    "#             print(self.delta.reshape((self.delta.shape[0], -1)).shape)\n",
    "#             print(eta)\n",
    "            if self.regularizer is not None:\n",
    "                if self.regularizer[0] == 'l1':\n",
    "                    pass\n",
    "                elif self.regularizer[0] == 'l2':\n",
    "    #                 print('use l2-------')\n",
    "                    self.weight -= (eta * 2 * self.regularizer[1] * self.weight)\n",
    "                    self.bias -= (eta * 2 * self.regularizer[1] * self.bias)\n",
    "            update_w = (self.x.T @ self.delta) / self.x.shape[0]\n",
    "            self.weight -= (eta * update_w)\n",
    "#             print(self.bias.shape)\n",
    "#             print(self.delta.shape)\n",
    "            update_b = np.sum(self.delta, axis=0) / self.x.shape[0]\n",
    "            self.bias -= (eta * update_b)\n",
    "            assert not True in np.isnan(self.weight) and not True in np.isnan(self.bias), \"NaN error during weights update!!!\"\n",
    "#             print(np.sum(self.delta,axis=0).shape)\n",
    "    \n",
    "    def network(self, net_conf):\n",
    "        assert len(net_conf['architecture'].keys()) >= 2, \"Network Architecture Error!!\"\n",
    "        self.net = {}\n",
    "        for l in net_conf['architecture']:\n",
    "            self.net[l] = self.layer(net_conf['architecture'][l][0], net_conf['architecture'][l][1], net_conf['architecture'][l][2], net_conf['training']['regularizer'])\n",
    "        self.net_conf = net_conf\n",
    "#         self.net['layer1'] = self.layer(784, 512, 'ReLU')\n",
    "#         self.net['layer2'] = self.layer(512, 512, 'ReLU')\n",
    "#         self.net['layer3'] = self.layer(512, 10, 'softmax')\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "#         print(x)\n",
    "        for n in self.net:\n",
    "            print('forward', n)\n",
    "            x = self.net[n].forward(x)\n",
    "#             print(x)\n",
    "        return x\n",
    "    \n",
    "    def backpropagation(self, y_hat, y, eta):\n",
    "        delta = y_hat - y\n",
    "        print(delta.shape)\n",
    "#         assert 0, 'test'\n",
    "        for n in reversed(list(self.net.keys())):\n",
    "            delta = self.net[n].backward(delta)\n",
    "        # update parameters\n",
    "        for n in self.net:\n",
    "            self.net[n].update(eta)\n",
    "\n",
    "    def cost(self, y_hat, y):\n",
    "        # cross entropy\n",
    "        return -np.sum(y * np.log(y_hat))\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        y_hat = self.feedforward(x)\n",
    "        pred = np.argmax(y_hat, axis=1)\n",
    "        y_ = np.argmax(y, axis=1)\n",
    "        print(pred.shape)\n",
    "        return np.sum(pred == y_) / pred.shape[0]\n",
    "\n",
    "    def sgd(self, eta=0.001, epoch=50, minibatch=500, regularizer=None, momentum=False, decay=False):\n",
    "        # SGD\n",
    "        iteration = self.training['xdata'].shape[0] // minibatch\n",
    "        print(\"<====start training====>\")\n",
    "        print(\"batch size is\", minibatch)\n",
    "        self.train_acc = []\n",
    "        self.valid_acc = []\n",
    "        self.test_acc = []\n",
    "        self.decay_pts = []\n",
    "        for i in range(epoch):\n",
    "            # 1. randomly shuffle\n",
    "            p = np.random.permutation(self.training['xdata'].shape[0])\n",
    "#             print(p)\n",
    "            for j in range(iteration):\n",
    "                string = \"\\r[epoch %d/%d] iteration#%d \" % (i + 1, epoch, j + 1)\n",
    "                print(self.training['xdata'][j * batch_size:(j + 1) * batch_size].shape)\n",
    "                print(self.training['xdata'][p][j * minibatch:(j + 1) * minibatch].shape)\n",
    "                y_hat = self.feedforward(self.training['xdata'][p][j * minibatch:(j + 1) * minibatch])\n",
    "#                 print(y_hat.shape)\n",
    "                self.backpropagation(y_hat, self.training['ydata'][p][j * minibatch:(j + 1) * minibatch], eta)\n",
    "                print(\"err:\", err)\n",
    "#                 acc = self.accuracy(self.training['xdata'][p][j * batch_size:(j + 1) * batch_size], self.training['ydata'][p][j * batch_size:(j + 1) * batch_size])\n",
    "                if bool(self.validation) is False:\n",
    "                    acc = self.accuracy(self.test['xdata'], self.test['ydata'])\n",
    "                    print(string + \"test acc: %lf \" % (acc), end='')\n",
    "                else:\n",
    "                    print('here????')\n",
    "                    v_acc = self.accuracy(self.validation['xdata'], self.validation['ydata'])\n",
    "                    print(string + \"valid acc: %lf \" % (v_acc), end='')\n",
    "            t_acc = self.accuracy(self.training['xdata'], self.training['ydata'])\n",
    "            self.train_acc.append(t_acc)\n",
    "            if bool(self.validation) is False:\n",
    "                acc = self.accuracy(self.test['xdata'], self.test['ydata'])\n",
    "                self.test_acc.append(acc)\n",
    "                print(\"[epoch end] train acc: %lf, test acc: %lf \" % (t_acc, acc), end='')\n",
    "            else:\n",
    "                v_acc = self.accuracy(self.validation['xdata'], self.validation['ydata'])\n",
    "                self.valid_acc.append(v_acc)\n",
    "                print(\"[epoch end] train acc: %lf, valid acc: %lf \" % (t_acc, v_acc), end='')\n",
    "            if decay is True and (i + 1) % int(np.ceil(epoch / 3)) == 0:\n",
    "                eta /= 2.0\n",
    "                self.decay_pts.append(i)\n",
    "                print(\"Learning rate decay by 2: %lf\" % (eta), end='')\n",
    "            print('')\n",
    "                break\n",
    "            break\n",
    "\n",
    "    def setup(self, net_conf):\n",
    "        self.network(net_conf)\n",
    "\n",
    "    def train(self, net_conf):\n",
    "        if net_conf['training']['optimizer'] == 'sgd':\n",
    "            self.sgd(net_conf['training']['eta'], net_conf['training']['epoch'],\n",
    "                     net_conf['training']['minibatch'], net_conf['training']['regularizer'],\n",
    "                     net_conf['training']['momentum'], net_conf['training']['decay'])\n",
    "            \n",
    "    def save(self, filename):\n",
    "        with h5py.File(filename, 'w') as hf:\n",
    "            for n in self.net:\n",
    "                hf.create_dataset(n + '_w', data=self.net[n].weight)\n",
    "                hf.create_dataset(n + '_b', data=self.net[n].bias)\n",
    "            hf.create_dataset('train_acc', data=self.train_acc)\n",
    "            hf.create_dataset('valid_acc', data=self.valid_acc)\n",
    "            hf.create_dataset('test_acc', data=self.test_acc)\n",
    "            hf.create_dataset('decay_pts', data=self.decay_pts)\n",
    "                \n",
    "    def load(self, filename):\n",
    "        with h5py.File(filename, 'r') as hf:\n",
    "            for n in self.net:\n",
    "                self.net[n].set_par(hf[n + '_w'][:], hf[n + '_b'][:])\n",
    "            self.train_acc = hf['train_acc'][:]\n",
    "            self.valid_acc = hf['valid_acc'][:]\n",
    "            self.test_acc = hf['test_acc'][:]\n",
    "            self.decay_pts = hf['decay_pts'][:]\n",
    "\n",
    "        \n",
    "mlp = MLP('mnist_traindata.hdf5', 'mnist_testdata.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp.optimizer(eta=0.01)\n",
    "net_conf = {\n",
    "    'architecture': {\n",
    "        #'layer1': input layer,\n",
    "        'layer2': [784, 48, 'ReLU'],\n",
    "        'layer3': [48, 10, 'softmax'],\n",
    "    },\n",
    "    'training': {\n",
    "        'eta': 1.0,\n",
    "        'optimizer': 'sgd',\n",
    "        'epoch': 50,\n",
    "        'minibatch': 500,\n",
    "        'regularizer': ['l2', 0.001],\n",
    "        'momentum': False,\n",
    "        'decay': True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====start training====>\n",
      "batch size is 500\n",
      "[epoch 1/50] iteration#120 test acc: 0.924000 [epoch end] train acc: 0.922167, test acc: 0.924000 \n",
      "[epoch 2/50] iteration#120 test acc: 0.943800 [epoch end] train acc: 0.941667, test acc: 0.943800 \n",
      "[epoch 3/50] iteration#120 test acc: 0.948600 [epoch end] train acc: 0.949783, test acc: 0.948600 \n",
      "[epoch 4/50] iteration#120 test acc: 0.952000 [epoch end] train acc: 0.953050, test acc: 0.952000 \n",
      "[epoch 5/50] iteration#120 test acc: 0.957900 [epoch end] train acc: 0.959550, test acc: 0.957900 \n",
      "[epoch 6/50] iteration#120 test acc: 0.957400 [epoch end] train acc: 0.960983, test acc: 0.957400 \n",
      "[epoch 7/50] iteration#120 test acc: 0.950800 [epoch end] train acc: 0.956083, test acc: 0.950800 \n",
      "[epoch 8/50] iteration#120 test acc: 0.958400 [epoch end] train acc: 0.960200, test acc: 0.958400 \n",
      "[epoch 9/50] iteration#120 test acc: 0.961100 [epoch end] train acc: 0.963417, test acc: 0.961100 \n",
      "[epoch 10/50] iteration#120 test acc: 0.962000 [epoch end] train acc: 0.965517, test acc: 0.962000 \n",
      "[epoch 11/50] iteration#120 test acc: 0.958700 [epoch end] train acc: 0.961833, test acc: 0.958700 \n",
      "[epoch 12/50] iteration#120 test acc: 0.964000 [epoch end] train acc: 0.965433, test acc: 0.964000 \n",
      "[epoch 13/50] iteration#120 test acc: 0.961900 [epoch end] train acc: 0.963067, test acc: 0.961900 \n",
      "[epoch 14/50] iteration#120 test acc: 0.957500 [epoch end] train acc: 0.959717, test acc: 0.957500 \n",
      "[epoch 15/50] iteration#120 test acc: 0.961800 [epoch end] train acc: 0.964533, test acc: 0.961800 \n",
      "[epoch 16/50] iteration#120 test acc: 0.964300 [epoch end] train acc: 0.966917, test acc: 0.964300 \n",
      "[epoch 17/50] iteration#120 test acc: 0.961300 [epoch end] train acc: 0.965100, test acc: 0.961300 Learning rate decay by 2: 0.500000\n",
      "[epoch 18/50] iteration#120 test acc: 0.967800 [epoch end] train acc: 0.971517, test acc: 0.967800 \n",
      "[epoch 19/50] iteration#120 test acc: 0.966700 [epoch end] train acc: 0.970800, test acc: 0.966700 \n",
      "[epoch 20/50] iteration#120 test acc: 0.967400 [epoch end] train acc: 0.971350, test acc: 0.967400 \n",
      "[epoch 21/50] iteration#120 test acc: 0.968000 [epoch end] train acc: 0.971717, test acc: 0.968000 \n",
      "[epoch 22/50] iteration#120 test acc: 0.968300 [epoch end] train acc: 0.972083, test acc: 0.968300 \n",
      "[epoch 23/50] iteration#120 test acc: 0.968100 [epoch end] train acc: 0.972150, test acc: 0.968100 \n",
      "[epoch 24/50] iteration#120 test acc: 0.968300 [epoch end] train acc: 0.973350, test acc: 0.968300 \n",
      "[epoch 25/50] iteration#120 test acc: 0.967300 [epoch end] train acc: 0.971983, test acc: 0.967300 \n",
      "[epoch 26/50] iteration#120 test acc: 0.968800 [epoch end] train acc: 0.972517, test acc: 0.968800 \n",
      "[epoch 27/50] iteration#120 test acc: 0.968800 [epoch end] train acc: 0.973317, test acc: 0.968800 \n",
      "[epoch 28/50] iteration#120 test acc: 0.968700 [epoch end] train acc: 0.973300, test acc: 0.968700 \n",
      "[epoch 29/50] iteration#120 test acc: 0.968100 [epoch end] train acc: 0.972483, test acc: 0.968100 \n",
      "[epoch 30/50] iteration#120 test acc: 0.969400 [epoch end] train acc: 0.973133, test acc: 0.969400 \n",
      "[epoch 31/50] iteration#120 test acc: 0.968800 [epoch end] train acc: 0.974317, test acc: 0.968800 \n",
      "[epoch 32/50] iteration#120 test acc: 0.968200 [epoch end] train acc: 0.972667, test acc: 0.968200 \n",
      "[epoch 33/50] iteration#120 test acc: 0.967400 [epoch end] train acc: 0.972967, test acc: 0.967400 \n",
      "[epoch 34/50] iteration#120 test acc: 0.969500 [epoch end] train acc: 0.973483, test acc: 0.969500 Learning rate decay by 2: 0.250000\n",
      "[epoch 35/50] iteration#120 test acc: 0.970000 [epoch end] train acc: 0.975067, test acc: 0.970000 \n",
      "[epoch 36/50] iteration#120 test acc: 0.970300 [epoch end] train acc: 0.974167, test acc: 0.970300 \n",
      "[epoch 37/50] iteration#120 test acc: 0.969000 [epoch end] train acc: 0.974867, test acc: 0.969000 \n",
      "[epoch 38/50] iteration#120 test acc: 0.970200 [epoch end] train acc: 0.974983, test acc: 0.970200 \n",
      "[epoch 39/50] iteration#120 test acc: 0.969600 [epoch end] train acc: 0.974817, test acc: 0.969600 \n",
      "[epoch 40/50] iteration#120 test acc: 0.970100 [epoch end] train acc: 0.975050, test acc: 0.970100 \n",
      "[epoch 41/50] iteration#120 test acc: 0.970000 [epoch end] train acc: 0.974950, test acc: 0.970000 \n",
      "[epoch 42/50] iteration#120 test acc: 0.969100 [epoch end] train acc: 0.974383, test acc: 0.969100 \n",
      "[epoch 43/50] iteration#120 test acc: 0.969300 [epoch end] train acc: 0.974117, test acc: 0.969300 \n",
      "[epoch 44/50] iteration#120 test acc: 0.970000 [epoch end] train acc: 0.974633, test acc: 0.970000 \n",
      "[epoch 45/50] iteration#120 test acc: 0.970300 [epoch end] train acc: 0.975200, test acc: 0.970300 \n",
      "[epoch 46/50] iteration#120 test acc: 0.970500 [epoch end] train acc: 0.975083, test acc: 0.970500 \n",
      "[epoch 47/50] iteration#120 test acc: 0.970300 [epoch end] train acc: 0.975350, test acc: 0.970300 \n",
      "[epoch 48/50] iteration#120 test acc: 0.970600 [epoch end] train acc: 0.975300, test acc: 0.970600 \n",
      "[epoch 49/50] iteration#120 test acc: 0.971000 [epoch end] train acc: 0.975850, test acc: 0.971000 \n",
      "[epoch 50/50] iteration#120 test acc: 0.969300 [epoch end] train acc: 0.975500, test acc: 0.969300 \n"
     ]
    }
   ],
   "source": [
    "mlp.setup(net_conf)\n",
    "mlp.train(net_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
